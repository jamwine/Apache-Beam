{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795b14b2",
   "metadata": {},
   "source": [
    "# Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc59c7e",
   "metadata": {},
   "source": [
    "**Apache Beam** is a unified programming model for defining both **batch** jobs and **streaming** data-parallel processing pipelines.\n",
    "\n",
    "<img src='imgs/beam_pipeline.svg' alt='beam_pipeline.svg' width=800 height=600>\n",
    "\n",
    "* A **pipeline** is a user-constructed graph of transformations that defines the desired data processing operations.\n",
    "\n",
    "\n",
    "* A **PCollection** is a data set or data stream. The data that a pipeline processes is part of a PCollection.\n",
    "\n",
    "\n",
    "* A **PTransform** (or **transform**) represents a data processing operation, or a step, in our pipeline. A transform is applied to zero or more PCollection objects, and produces zero or more PCollection objects.\n",
    "\n",
    "\n",
    "* A **runner** runs a Beam pipeline using the capabilities of our chosen data processing engine. It is also known as an **execution engine**.\n",
    "\n",
    "\n",
    "* **SDK** is a language-specific library that lets pipeline authors build transforms, construct their pipelines, and submit them to a runner. Apache Beam follows the philosopy **\"Write once, run everywhere\"** and has SDK in multiple languages, i.e Python, Java and Go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install apache-beam\n",
    "# !pip install apache-beam[interactive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1ec601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb01f8",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75d6fc",
   "metadata": {},
   "source": [
    "A Beam pipeline is a graph (specifically, a **directed acyclic graph**) of all the data and computations in our data processing task. This includes reading input data, transforming that data, and writing output data. \n",
    "\n",
    "A pipeline is constructed by a user in their **SDK** of choice. Then, the pipeline makes its way to the **runner** either through the SDK directly or through the Runner API’s RPC interface. For example, this diagram shows a branching pipeline:\n",
    "\n",
    "<img src='imgs/pipeline.svg' alt='pipeline.svg' width=800 height=600>\n",
    "\n",
    "\n",
    "We can run a Beam pipeline locally using the **DirectRunner**, it's useful to for debugging and testing. However in production, we want to run the pipeline into a remote executor for maximum performance and potentially scaling capabilities. Normally, cloud providers are utilized to simulate critical parts of Apache Beam pipeline like streaming and storage. For example, on the Google Cloud Platform:\n",
    "* **Google Cloud Storage** buckets are used to store the data and the trained model.\n",
    "* **Google Cloud Pub/Sub** ingesting platform is used for event-driven systems and streaming analytics.\n",
    "* **Google Cloud DataFlow** is utilized as the cloud executor (runner) for job expressed with Apache Beam SDK.\n",
    "\n",
    "Another example of some Machine Learning pipeline utilizing Apache Beam can be something like this:\n",
    "\n",
    "<img src='imgs/beam.png' alt='beam.png' width=800 height=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d70cee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, "
     ]
    }
   ],
   "source": [
    "#  the entry point of pipeline using `with` clause\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    (pipeline\n",
    "     | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "     | \"print\" >> beam.Map(lambda x: print(x, end=', ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c81c3e",
   "metadata": {},
   "source": [
    "We use the **pipe** operator `|` to apply a transform on a collection. We can pair it with the `>>` to give a name to the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5675d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79586648",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A PCollection generally contains **big data** : it is an unordered bag of elements that might be **bounded**, **stored**, **datasets**, or **unbounded streams** of data. PCollections flows between the transforms. In Beam, most transforms apply equally to bounded and unbounded data.\n",
    "\n",
    "A Beam driver program typically starts by creating a Pipeline object, and then uses that object as the basis for creating the pipeline’s data sets and its transforms. Each PCollection is owned by the specific Pipeline object for which it is created and multiple pipelines cannot share a PCollection. \n",
    "\n",
    "Beam pipelines process PCollections, and the runner is responsible for storing these elements. Beam’s computational patterns and transforms are focused on situations where distributed data-parallel computation is required. Therefore, the elements of a PCollection cannot be processed individually, and are instead processed uniformly in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d8912",
   "metadata": {
    "cell_style": "center",
    "hidden": true
   },
   "source": [
    "### bounded PCollection\n",
    "A **bounded PCollection** is a dataset of a known, fixed size (alternatively, a dataset that is not growing over time). Bounded data can be processed by **batch pipelines**.\n",
    "\n",
    "### unbounded PCollection\n",
    "An **unbounded PCollection** is a dataset that grows over time, and the elements are processed as they arrive. Unbounded data must be processed by **streaming pipelines**.\n",
    "\n",
    "Both bounded and unbounded PCollections can coexist in the same pipeline. If our runner can only support bounded PCollections, we must reject pipelines that contain unbounded PCollections. If our runner is only targeting streams, there are adapters in Beam’s support code to convert everything to APIs that target unbounded data.\n",
    "\n",
    "<img src='imgs/batch-stream.png' alt='batch-stream.png' width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8fe334",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020d340",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A **PTransform** (or **transform**) represents a data processing operation, or a step, in our pipeline. A transform is usually applied to one or more input PCollection objects. Transforms that read input are an exception; these transforms might not have an input PCollection.\n",
    "\n",
    "We provide transform processing **logic** in the form of a function object (referred as **user code**), and our user code is applied to each element of the input PCollection (or more than one PCollection). Depending on the pipeline runner and backend that we choose, many different workers across a cluster might execute instances of our user code in parallel. The user code that runs on each worker generates the output elements that are added to zero or more output PCollection objects.\n",
    "\n",
    "The Beam SDKs contain a number of different transforms that we can apply to our pipeline’s PCollections. We can also define our own more complex composite transforms to fit our pipeline’s exact use case. The following list has some common transform types:\n",
    "\n",
    "* Source transforms such as **TextIO.Read** and **Create**. A source transform conceptually has no input.\n",
    "* Processing and conversion operations such as **ParDo**, **GroupByKey**, **CoGroupByKey**, **Combine**, and **Count**.\n",
    "* Outputting transforms such as **TextIO.Write**.\n",
    "* **User-defined**, application-specific composite transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65c43f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fd44a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A **Beam runner** runs a Beam pipeline on a specific platform. Most runners are translators or adapters to massively parallel big data processing systems, such as **Apache Flink**, **Apache Spark**, **Google Cloud Dataflo**w, and more. For example, the Flink runner translates a Beam pipeline into a Flink job. \n",
    "\n",
    "The **Direct Runner** runs pipelines locally so we can test, debug, and validate that our pipeline adheres to the Apache Beam model as closely as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2e478",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c0691",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Aggregation** is computing a value from multiple (1 or more) input elements. In Beam, the primary computational pattern for aggregation is to group all elements with a common key and window, then combine each group of elements using an associative and commutative operation. This is similar to the **Reduce** operation in the MapReduce model, though it is enhanced to work with unbounded input streams as well as bounded data sets.\n",
    "\n",
    "For example during aggregation, elements with the same color represent those with a common key and window.\n",
    "\n",
    "<img src='imgs/aggregation.png' alt='batch-stream.png' width=100 height=200>\n",
    "\n",
    "\n",
    "Some simple aggregation transforms include **Count** (computes the count of all elements in the aggregation), **Max** (computes the maximum element in the aggregation), and **Sum** (computes the sum of all elements in the aggregation).\n",
    "\n",
    "When elements are grouped and emitted as a bag, the aggregation is known as **GroupByKey** (the associative/commutative operation is bag union). In this case, the output is no smaller than the input. Often, we will apply an operation such as summation, called a **CombineFn**, in which the output is significantly smaller than the input. In this case the aggregation is called **CombinePerKey**.\n",
    "\n",
    "\n",
    "In a real application, we might have millions of keys and/or windows; that is why this is still an “embarassingly parallel” computational pattern. In those cases where we have fewer keys, we can add parallelism by adding a supplementary key, splitting each of our problem’s natural keys into many sub-keys. After these sub-keys are aggregated, the results can be further combined into a result for the original natural key for our problem. The associativity of our aggregation function ensures that this yields the same answer, but with more parallelism.\n",
    "\n",
    "When our input is unbounded, the computational pattern of grouping elements by key and window is roughly the same, but governing when and how to emit the results of aggregation involves three concepts:\n",
    "\n",
    "* **Windowing**, which partitions our input into bounded subsets that can be complete.\n",
    "\n",
    "\n",
    "* **Watermarks**, which estimate the completeness of our input.\n",
    "\n",
    "\n",
    "* **Triggers**, which govern when and how to emit aggregated results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a9dc6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## User-defined function (UDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242e2ca",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some Beam operations allow us to run user-defined code as a way to configure the transform. For example, when using **ParDo**, user-defined code specifies what operation to apply to every element. For Combine, it specifies how values should be combined. By using cross-language transforms, a Beam pipeline can contain **UDFs** written in a different language, or even multiple languages in the same pipeline.\n",
    "\n",
    "Beam has several varieties of UDFs:\n",
    "\n",
    "* **DoFn** - per-element processing function (used in **ParDo**)\n",
    "\n",
    "\n",
    "* **WindowFn** - places elements in windows and merges windows (used in Window and GroupByKey)\n",
    "\n",
    "\n",
    "* **ViewFn** - adapts a materialized PCollection to a particular interface (used in side inputs)\n",
    "\n",
    "\n",
    "* **WindowMappingFn** - maps one element’s window to another, and specifies bounds on how far in the past the result window will be (used in side inputs)\n",
    "\n",
    "\n",
    "* **CombineFn** - associative and commutative aggregation (used in Combine and state)\n",
    "\n",
    "\n",
    "* **Coder** - encodes user data; some coders have standard formats and are not really UDFs\n",
    "\n",
    "\n",
    "Each language SDK has its own idiomatic way of expressing the user-defined functions in Beam, but there are common requirements. When we build user code for a Beam transform, we should keep in mind the distributed nature of execution. For example, there might be many copies of our function running on a lot of different machines in parallel, and those copies function independently, without communicating or sharing state with any of the other copies. Each copy of our user code function might be retried or run multiple times, depending on the pipeline runner and the processing backend that we choose for our pipeline. \n",
    "\n",
    "Beam also supports stateful processing through the stateful processing API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee84e086",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376b990",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A **schema** is a language-independent type definition for a PCollection. The schema for a PCollection defines elements of that PCollection as an ordered list of named fields. Each field has a name, a type, and possibly a set of user options.\n",
    "\n",
    "The element type in a PCollection has a structure that can be introspected. For example, formats like **JSON**, **Protocol Buffer**, **Avro**, and **database row objects** can be converted to Beam Schemas. By understanding the structure of a pipeline’s records, we can define APIs for data processing.\n",
    "\n",
    "Beam provides a collection of transforms that operate natively on schemas. For example, **Beam SQL** is a common transform that operates on schemas. These transforms allow selections and aggregations in terms of named **schema fields**. Another advantage of schemas is that they allow referencing of element fields by name. Beam provides a selection syntax for referencing fields, including nested and repeated fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e572d13",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454157c7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Every element in a PCollection has a **timestamp** associated with it. When we execute a primitive connector to a storage system, that connector is responsible for providing initial timestamps. \n",
    "\n",
    "The runner must propagate and aggregate timestamps. If the timestamp is not important, such as with certain batch processing jobs where elements do not denote events, the timestamp will be the minimum representable timestamp, often referred to colloquially as **negative infinity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43be0f8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9bf058",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A PCollection can be subdivided into **windows** based on the **timestamps** of the individual elements. Windows enable grouping operations over unbounded collections that grow over time by dividing the collection into windows of finite collections.\n",
    "\n",
    "A windowing function tells the runner how to assign elements to one or more initial windows, and how to merge windows of grouped elements. **Each element in a PCollection can only be in one window**, so if a windowing function specifies multiple windows for an element, the element is conceptually duplicated into each of the windows and **each element is identical except for its window**.\n",
    "\n",
    "Transforms that aggregate multiple elements, such as **GroupByKey** and **Combine**, work implicitly on a per-window basis; they process each PCollection as a succession of multiple, finite windows, though the entire collection itself may be of unbounded size.\n",
    "\n",
    "Beam provides several windowing functions:\n",
    "\n",
    "* **Fixed time windows** (also known as **tumbling windows**) represent a consistent duration, non-overlapping time interval in the data stream.\n",
    "\n",
    "\n",
    "* **Sliding time windows** (also known as **hopping windows**) also represent time intervals in the data stream; however, sliding time windows can overlap.\n",
    "\n",
    "\n",
    "* **Per-session windows** define windows that contain elements that are within a certain gap duration of another element.\n",
    "\n",
    "\n",
    "* **Single global window**: by default, all data in a PCollection is assigned to the single global window, and late data is discarded.\n",
    "\n",
    "\n",
    "* **Calendar-based windows** (not supported by the Beam SDK for Python)\n",
    "\n",
    "\n",
    "* **Customized windows** are user-defined windows, based on complex requirements.\n",
    "\n",
    "For example, let’s say we have a PCollection that uses **fixed-time windowing**, with windows that are five minutes long. For each window, Beam must collect all the data with an event time timestamp in the given window range (between 0:00 and 4:59 in the first window, for instance). Data with timestamps outside that range (data from 5:00 or later) belongs to a different window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b84e09",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe872f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In any data processing system, there is a certain amount of lag between the time a data event occurs (the **event time**, determined by the timestamp on the data element itself) and the time the actual data element gets processed at any stage in your pipeline (the **processing time**, determined by the clock on the system processing the element). In addition, data isn’t always guaranteed to arrive in a pipeline in time order, or to always arrive at predictable intervals. For example, we might have intermediate systems that don’t preserve order, or we might have two servers that timestamp data but one has a better network connection.\n",
    "\n",
    "A **watermark** is a guess as to when all data in a certain window is expected to have arrived. This is needed because data isn’t always guaranteed to arrive in a pipeline in time order, or to always arrive at predictable intervals. A window has a maximum timestamp. When the watermark exceeds the maximum timestamp plus the user-specified allowed lateness, the window is expired. All data related to an expired window might be discarded at any time.\n",
    "\n",
    "**Data sources are responsible for producing a watermark**, and **every PCollection must have a watermark** that estimates how complete the PCollection is. The runner must implement watermark propagation as PCollections are processed, merged, and partitioned. The contents of a PCollection are complete when a watermark advances to **infinity**. In this manner, we might discover that an unbounded PCollection is finite. After the watermark progresses past the end of a window, any further element that arrives with a timestamp in that window is considered **late data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe15e2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0318d0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Triggers** are a related concept that allow us to modify and refine the **windowing strategy** for a PCollection. We can use triggers to decide when each individual window aggregates and reports its results, including how the window emits late elements.\n",
    "\n",
    "When collecting and grouping data into windows, Beam uses triggers to determine when to emit the aggregated results of each **window** (referred to as a **pane**). If we use Beam’s default windowing configuration and default trigger, Beam outputs the aggregated result when it estimates all data has arrived, and discards all subsequent data for that window.\n",
    "\n",
    "Triggers also provide two additional capabilities that allow us to control the flow of our data and also balance between **data completeness**, **latency**, and **cost**:\n",
    "\n",
    "* Triggers allow Beam to **emit early results**, before all the data in a given window has arrived. For example, emitting after a certain amount of time elapses, or after a certain number of elements arrives.\n",
    "\n",
    "\n",
    "* Triggers allow **processing of late data** by triggering after the event time watermark passes the end of the window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d94505",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Beam provides a number of pre-built triggers that we can set:\n",
    "\n",
    "* **Event time triggers**: These triggers operate on the event time, as indicated by the timestamp on each data element. Beam’s default trigger is event time-based.\n",
    "\n",
    "\n",
    "* **Processing time triggers**: These triggers operate on the processing time, which is the time when the data element is processed at any given stage in the pipeline.\n",
    "\n",
    "\n",
    "* **Data-driven triggers**: These triggers operate by examining the data as it arrives in each window, and firing when that data meets a certain property. Currently, data-driven triggers only support firing after a certain number of data elements.\n",
    "\n",
    "\n",
    "* **Composite triggers**: These triggers combine multiple triggers in various ways. For example, we might want one trigger for early data and a different trigger for late data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4f7d0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## State and timers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346be91b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Beam’s **windowing** and **triggers** provide an abstraction for grouping and aggregating unbounded input data based on **timestamps**. However, there are aggregation use cases that might require an even higher degree of control. **State** and **timers** are two important concepts that help with these uses cases. Per-key **state** and **timer** callbacks are lower level primitives that give us full control over aggregating input collections that grow over time.\n",
    "\n",
    "### State\n",
    "\n",
    "Beam provides the **State API** for manually managing per-key state, **allowing for fine-grained control over aggregations**. The State API lets us augment element-wise operations (for example, **ParDo** or **Map**) with mutable state. Like other aggregations, **state is processed per window**.\n",
    "\n",
    "The State API models **state per key**. To use the state API, we start out with a keyed PCollection. A ParDo that processes this PCollection can declare persistent state variables. When we process each element inside the ParDo, we can use the state variables to write or update state for the current key or to read previous state written for that key. State is always fully scoped only to the current processing key.\n",
    "\n",
    "Beam provides several types of state, though different runners might support a different subset of these states:\n",
    "\n",
    "* **ValueState**: ValueState is a scalar state value. For each key in the input, a ValueState stores a typed value that can be read and modified inside the DoFn.\n",
    "\n",
    "\n",
    "* A common use case for state is to accumulate multiple elements into a group:\n",
    "    \n",
    "    * **BagState**: BagState allows us to accumulate elements in an unordered bag. This lets us add elements to a collection without needing to read any of the previously accumulated elements.\n",
    "    * **MapState**: MapState allows us to accumulate elements in a map.\n",
    "    * **SetState**: SetState allows us to accumulate elements in a set.\n",
    "    * **OrderedListState**: OrderedListState allows us to accumulate elements in a timestamp-sorted list.\n",
    "\n",
    "\n",
    "* **CombiningState**: CombiningState allows us to create a state object that is updated using a Beam combiner. Like BagState, we can add elements to an aggregation without needing to read the current value, and the accumulator can be compacted using a combiner.\n",
    "\n",
    "### Timers\n",
    "\n",
    "Beam provides a per-key timer callback API that **enables delayed processing of data** stored using the State API. The **Timer API** lets us set timers to call back at either an **event-time** or a **processing-time** timestamp. For more advanced use cases, our timer callback can set another timer. Like other aggregations, **timers are processed per window**. \n",
    "\n",
    "The following timers are available:\n",
    "\n",
    "* **Event-time timers**: Event-time timers fire when the **input watermark for the DoFn passes the time at which the timer is set**, meaning that the runner believes that there are no more elements to be processed with timestamps before the timer timestamp. This allows for event-time aggregations.\n",
    "\n",
    "\n",
    "* **Processing-time timers**: Processing-time timers fire when the **real wall-clock time passes**. This is often used to create larger batches of data before processing. It can also be used to schedule events that should occur at a specific time.\n",
    "\n",
    "\n",
    "* **Dynamic timer tag**s: Beam also supports dynamically setting a timer tag. This allows us to set multiple different timers in a DoFn and dynamically choose timer tags (for example, based on data in the input elements).\n",
    "\n",
    "We can use the **State API** together with the **Timer API** to create processing tasks that give us fine-grained control over the workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a06818",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Splittable DoFn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48484e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Splittable DoFn** (**SDF**) is a generalization of **DoFn** that lets us process elements in a **non-monolithic** way. Splittable DoFn makes it easier to create complex, modular I/O connectors in Beam.\n",
    "\n",
    "A regular **ParDo** processes an entire element at a time, applying our regular DoFn and waiting for the call to terminate. When we instead apply a splittable DoFn to each element, the runner has the option of splitting the element’s processing into smaller tasks. We can checkpoint the processing of an element, and we can split the remaining work to yield additional parallelism.\n",
    "\n",
    "For example, imagine we want to read every line from very large text files. When we write our splittable DoFn, we can have separate pieces of logic to read a segment of a file, split a segment of a file into sub-segments, and report progress through the current segment. The runner can then invoke our splittable DoFn intelligently to split up each input and read portions separately, in parallel.\n",
    "\n",
    "A common computation pattern has the following steps:\n",
    "\n",
    "* The runner splits an incoming element before starting any processing.\n",
    "* The runner starts running our processing logic on each sub-element.\n",
    "* If the runner notices that some sub-elements are taking longer than others, the runner splits those sub-elements further and repeats step 2.\n",
    "* The sub-element either finishes processing, or the user chooses to checkpoint the sub-element and the runner repeats step 2.\n",
    "\n",
    "We can also write our splittable DoFn so the runner can split the unbounded processing. For example, if we write a splittable DoFn to watch a set of directories and output filenames as they arrive, we can split to subdivide the work of different directories. This allows the runner to split off a hot directory and give it additional resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf692597",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Coder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd217127",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Every PCollection has a **coder**, which is a specification of the **binary format** of the elements.\n",
    "\n",
    "In Beam, the user’s pipeline can be written in a language other than the language of the runner. There is no expectation that the runner can actually deserialize user data. The Beam model operates principally on encoded data, “just bytes”. Each PCollection has a declared encoding for its elements, called a **coder**. A coder has a **URN** that identifies the encoding, and might have additional **sub-coders**. For example, a coder for lists might contain a coder for the elements of the list. Language-specific serialization techniques are frequently used, but there are a few common key formats (such as key-value pairs and timestamps) so the runner can understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47cc22",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
